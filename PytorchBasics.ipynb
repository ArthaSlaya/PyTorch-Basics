{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking PyTorch Version and GPU Availability**"
      ],
      "metadata": {
        "id": "G7LnKNdaZfvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **1. Checking PyTorch Version**\n",
        "- You can check the installed PyTorch version using:\n",
        "  ```python\n",
        "  import torch\n",
        "  torch.__version__\n",
        "  ```\n",
        "- **Output**\n",
        "\n",
        "  ```python\n",
        "  2.5.1+cu124\n",
        "  ```\n",
        "\n",
        "## **2. Checking for GPU Availability**\n",
        "- PyTorch provides `torch.cuda.is_available()` to check if a **GPU** is available.\n",
        "- If a **GPU** is found, the script will print the **GPU name** and set `device` to `'cuda'`.\n",
        "- If no **GPU** is available, it defaults to using the **CPU**.\n",
        "\n",
        "### **Example**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')  # Set device to GPU\n",
        "else:\n",
        "    print(\"GPU is not available. Using CPU...\")\n",
        "    device = torch.device('cpu')   # Set device to CPU\n",
        "```\n",
        "- **Output**\n",
        "\n",
        "  ```python\n",
        "  GPU is available!\n",
        "  Using GPU: Tesla T4\n",
        "  ```\n",
        "\n",
        "## 3. **Creating a Tensor with Specified Values**\n",
        "  - You can create a manually specified tensor using `torch.tensor()`.\n",
        "\n",
        "  - **Example**\n",
        "  ```python\n",
        "  tensor_specified = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "  print(\"Specified tensor:\", tensor_specified)\n",
        "  ```\n",
        "\n",
        "  - **Output**\n",
        "  ```python\n",
        "  Specified tensor: tensor([[1, 2, 3],\n",
        "                          [4, 5, 6]])\n",
        "  ```\n",
        "\n",
        "## 4. **Moving a Tensor to GPU (if available)**\n",
        "- The `.to(device)` method moves a tensor to a specified device (CPU or GPU).\n",
        "  - **Example**\n",
        "  ```python\n",
        "  gpu_tensor = tensor_specified.to(device)\n",
        "  print(\"Tensor moved to:\", device)\n",
        "  ```\n",
        "  - **Output**\n",
        "  Tensor moved to: cuda\n",
        "\n"
      ],
      "metadata": {
        "id": "hzTg8yYGZfsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Tensors in PyTorch**"
      ],
      "metadata": {
        "id": "9Lqw7aZ5LnH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Creating an Uninitialized Tensor**\n",
        "### **Using `torch.empty()`**\n",
        "- Creates a **tensor with uninitialized values** (random garbage values in memory).\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.empty(size)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "torch.empty(2, 3)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[5.2421e-13, 4.3358e-41, 5.2421e-13],\n",
        "        [4.3358e-41, 4.4842e-44, 0.0000e+00]])\n",
        "```\n",
        "\n",
        "## **2. Creating Tensors with Specific Values**\n",
        "### **Using `torch.ones()`**\n",
        "- Creates a tensor filled with zeros.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.ones(size)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "torch.ones(2, 3)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[1., 1., 1.],\n",
        "        [1., 1., 1.]])\n",
        "```\n",
        "\n",
        "\n",
        "### **Using `torch.zeros()`**\n",
        "- Creates a tensor filled with ones.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.zeros(size)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "torch.zeros(2, 3)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[0., 0., 0.],\n",
        "        [0., 0., 0.]])\n",
        "```\n",
        "\n",
        "### **Using `torch.rand()`**\n",
        "- Creates a tensor filled with random values between `0` and `1`.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.rand(size)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "torch.rand(2, 3)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[0.2876, 0.3906, 0.0459],\n",
        "        [0.7114, 0.7884, 0.8450]])\n",
        "```\n",
        "\n",
        "\n",
        "## **3. Controlling Randomness with Seeds**\n",
        "### **Using torch.manual_seed()**\n",
        "- Sets the seed for reproducibility, ensuring that random values are the same each time.\n",
        "\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.manual_seed(seed_value)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "# Example of using a manual seed for reproducibility\n",
        "# First tensor\n",
        "torch.manual_seed(100)\n",
        "rand_tensor_1 = torch.rand(size=(2, 3))\n",
        "print(\"Random tensor with seed 100:\\n\", rand_tensor_1)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "Random tensor with seed 100:\n",
        " tensor([[0.1117, 0.8158, 0.2626],\n",
        "        [0.4839, 0.6765, 0.7539]])\n",
        "```\n",
        "\n",
        "- **Example**\n",
        "```python\n",
        "# Second Tensor\n",
        "torch.manual_seed(100)\n",
        "rand_tensor_2 = torch.rand(size=(2, 3))\n",
        "print(\"Random tensor with seed 100 again:\\n\", rand_tensor_2)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "Random tensor with seed 100 again:\n",
        " tensor([[0.1117, 0.8158, 0.2626],\n",
        "        [0.4839, 0.6765, 0.7539]])\n",
        "```\n",
        "\n",
        "## **4. Creating a Tensor with Specific Values**\n",
        "### **Using `torch.tensor()`**\n",
        "- Manually defines a tensor with specific values.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.tensor(data)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[1, 2, 3],\n",
        "        [4, 5, 6]])\n",
        "```\n",
        "\n",
        "## **5. Creating Tensors with Ranges**\n",
        "\n",
        "### **Using `torch.arange()`**\n",
        "- Creates a **sequence** of numbers with a given step size.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.arange(start, end, step)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "torch.arange(0, 10, 2)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([0, 2, 4, 6, 8])\n",
        "```\n",
        "\n",
        "### **Using `torch.linspace()`**\n",
        "- Creates a **sequence** of evenly spaced numbers between `start` and `end`.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.linspace(start, end, steps)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "  torch.linspace(0, 10, 10)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([ 0.0000,  1.1111,  2.2222,  3.3333,  4.4444,  \n",
        "         5.5556,  6.6667,  7.7778,  8.8889, 10.0000])\n",
        "```\n",
        "\n",
        "## **6. Creating Identity Matrices**\n",
        "\n",
        "### **Using `torch.eye()`**\n",
        "- Creates an **identity matrix** (a square matrix with `1s` on the diagonal and `0s` elsewhere).\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.eye(n)\n",
        "  ```\n",
        "- **Example**\n",
        "```python\n",
        "  torch.eye(5)\n",
        "```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[1., 0., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0.],\n",
        "        [0., 0., 1., 0., 0.],\n",
        "        [0., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 0., 1.]])\n",
        "```\n",
        "\n",
        "## **7. Creating a Tensor with a Fixed Value**\n",
        "\n",
        "### **Using `torch.full()`**\n",
        "- Creates a tensor filled with a **specified value**.\n",
        "- **Syntax:**  \n",
        "  ```python\n",
        "  torch.full(size, fill_value)\n",
        "```\n",
        "- **Example**\n",
        "```python\n",
        "  torch.full((3, 3), 5)\n",
        "  ```\n",
        "- **Output:**\n",
        "```python\n",
        "tensor([[5, 5, 5],\n",
        "        [5, 5, 5],\n",
        "        [5, 5, 5]])\n",
        "```\n",
        "## **Key Takeaways**\n",
        "✔ **`torch.empty()` creates a tensor with uninitialized values (random garbage).**  \n",
        "✔ **`torch.zeros()` and `torch.ones()` create tensors filled with zeros and ones, respectively.**  \n",
        "✔ **`torch.rand()` generates random values between `0` and `1`.**  \n",
        "✔ **`torch.manual_seed(seed_value)` ensures reproducibility by fixing random values.**  \n",
        "✔ **`torch.tensor()` is used for manually specifying values in a tensor.**  \n",
        "✔ **`torch.arange()` creates a range of numbers with a given step.**  \n",
        "✔ **`torch.linspace()` generates evenly spaced values between a start and end point.**  \n",
        "✔ **`torch.eye()` creates an identity matrix, useful in linear algebra.**  \n",
        "✔ **`torch.full()` initializes a tensor with a constant value.**  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZwnPj3aLnEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensor Shape and Initialization in PyTorch**"
      ],
      "metadata": {
        "id": "Nk2beT4o5wt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions allow you to **create new tensors** with certain properties while ensuring that the newly created tensor has the **same shape** as an existing one.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why do we need these functions?**\n",
        "- When working with tensors in **machine learning** or **deep learning**, we often need **new tensors** that have the **same shape** as an existing tensor but with different values (zeros, ones, random values, etc.).\n",
        "- Instead of **manually specifying the shape** every time, PyTorch provides functions like `empty_like()`, `zeros_like()`, `ones_like()`, and `rand_like()` to quickly create new tensors that **inherit the shape** of another tensor.\n",
        "\n",
        "---\n",
        "\n",
        "## **What is \"shape\" in a tensor?**\n",
        "- **Shape** defines the number of elements in each dimension of a tensor.\n",
        "- For example, a tensor with **2 rows and 3 columns** has the shape **(2,3)**.\n",
        "- Knowing the shape of tensors is essential for performing operations like matrix multiplication, reshaping, and data manipulation.\n",
        "\n",
        "```python\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(x.shape) # (2, 3)\n",
        "```\n",
        "\n",
        "## **How do these functions help?**\n",
        "- Suppose we have a tensor **x** with shape **(2,3)**, and we want:\n",
        "  - A tensor of the **same shape** but filled with **zeros** → `torch.zeros_like(x)`\n",
        "  - A tensor of the **same shape** but filled with **ones** → `torch.ones_like(x)`\n",
        "  - A tensor of the **same shape** but filled with **random values** → `torch.rand_like(x)`\n",
        "\n",
        "Instead of manually specifying the shape (`torch.zeros((2,3))`), we can simply call:\n",
        "\n",
        "```python\n",
        "torch.zeros_like(x)  # Automatically takes the shape of x\n",
        "```\n",
        "\n",
        "## **How do these functions work?**\n",
        "Each function takes a **reference tensor** (an existing tensor) and returns a **new tensor** with the same shape but different values.\n",
        "\n",
        "### **Function Breakdown**\n",
        "| **Function** | **What it does** | **Concept** |\n",
        "|-------------|----------------|------------|\n",
        "| `x.shape` | Returns tensor shape | Used to check the dimensions of a tensor |\n",
        "| `torch.empty_like(x)` | Creates a tensor with **random uninitialized values** | Placeholder tensor (values are not initialized) |\n",
        "| `torch.zeros_like(x)` | Creates a tensor filled with **zeros** | Useful for initializing tensors before calculations |\n",
        "| `torch.ones_like(x)` | Creates a tensor filled with **ones** | Commonly used for bias initialization |\n",
        "| `torch.rand_like(x, dtype=torch.float32)` | Creates a tensor filled with **random values between 0 and 1** | Used for weight initialization in deep learning |\n",
        "\n",
        "## **Real-World Use Cases**\n",
        "\n",
        "### **1. Initializing Weights in Neural Networks**\n",
        "- In deep learning, weight matrices are often initialized with **random values**.\n",
        "- Instead of manually defining a new tensor, `torch.rand_like(x)` creates a **random tensor** with the **same shape** as an existing one.\n",
        "\n",
        "```python\n",
        "input_tensor = torch.randn(4, 5)  # A layer with 4 neurons and 5 inputs\n",
        "weight_matrix = torch.rand_like(input_tensor)  # Random weights of same shape\n",
        "```\n",
        "\n",
        "### **2. Creating Masking Tensors**\n",
        "- In some applications, we need to create **binary masks** (tensors filled with `0`s and `1`s) for **feature selection** or **dropout layers** in neural networks.\n",
        "\n",
        "```python\n",
        "input_tensor = torch.randn(3, 3)\n",
        "mask = torch.ones_like(input_tensor)  # Create a mask of 1s for input\n",
        "```\n",
        "\n",
        "### **3. Reserving Memory Without Initializing Values**\n",
        "- `torch.empty_like(x)` is used when we need a **tensor placeholder** but will **fill it with values later**.\n"
      ],
      "metadata": {
        "id": "LLYC7H21z5DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a temporary tensor\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])"
      ],
      "metadata": {
        "id": "y0Ih6VVs31Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.empty_like(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo1d_eIU8wcV",
        "outputId": "1ca137fa-3136-4db1-949b-e2944f9d7493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7309453675965983778, 8315168162784306286, 8367752027310484831],\n",
              "        [7954801838398993778, 2459029315949324647, 7234581147814279472]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros_like(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQbRi9xJ84Fm",
        "outputId": "d89b7978-51e7-4c80-af64-d8381f67b4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones_like(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHsPSAX288dV",
        "outputId": "8743c950-fc68-4d7f-937f-f3fb97a5c868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1],\n",
              "        [1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why does `torch.rand_like()` require a floating-point dtype?**\n",
        "- `torch.rand_like()` generates **random values between 0 and 1**, which are **decimal (floating-point) numbers**.\n",
        "- Integer data types (like `torch.int32` or `torch.int64`) **cannot store decimal values**, so PyTorch **needs a float dtype** to properly create the random tensor.\n",
        "\n",
        "## **How to correctly use `torch.rand_like()`**\n",
        "- If the reference tensor (`x`) is already a **floating-point type**, `torch.rand_like(x)` works **directly**.\n",
        "- If the reference tensor is an **integer tensor**, you **must specify a floating-point dtype**.  \n",
        "\n",
        "## **Example Usage**\n",
        "```python\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)  # Integer tensor\n",
        "\n",
        "# This will raise an error because x is an integer tensor\n",
        "# rand_tensor = torch.rand_like(x)  ❌\n",
        "\n",
        "# Correct way: Specify dtype as a floating-point type\n",
        "rand_tensor = torch.rand_like(x, dtype=torch.float32)  # ✅\n",
        "print(rand_tensor)\n",
        "```\n",
        "\n",
        "**Expected Output: **A tensor of the same shape as x, but filled with random floating-point values between `0` and `1`.\n",
        "\n",
        "```python\n",
        "tensor([[0.1342, 0.8423, 0.5761],\n",
        "        [0.2983, 0.6512, 0.9134]])\n",
        "```\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **`torch.rand_like()` generates floating-point numbers** → Integer tensors **must** specify a floating-point dtype.  \n",
        "✔ **Valid floating-point dtypes:** `torch.float32`, `torch.float64`, `torch.float16`.  \n",
        "✔ **If the tensor is an integer type, PyTorch will raise an error unless you specify a valid dtype.**"
      ],
      "metadata": {
        "id": "kJMEJNS54q-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand_like(x, dtype= torch.float64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z2afIZH8-Je",
        "outputId": "0cb8f5f2-de63-4278-bd02-bf273ecf70fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1015, 0.6642, 0.9736],\n",
              "        [0.6941, 0.3464, 0.9751]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Data Types"
      ],
      "metadata": {
        "id": "AvQitHys57OI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Data Type**             | **Dtype**         | **Description**                                                                                                                                                                |\n",
        "|---------------------------|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **32-bit Floating Point** | `torch.float32`   | Standard floating-point type used for most deep learning tasks. Provides a balance between precision and memory usage.                                                         |\n",
        "| **64-bit Floating Point** | `torch.float64`   | Double-precision floating point. Useful for high-precision numerical tasks but uses more memory.                                                                               |\n",
        "| **16-bit Floating Point** | `torch.float16`   | Half-precision floating point. Commonly used in mixed-precision training to reduce memory and computational overhead on modern GPUs.                                            |\n",
        "| **BFloat16**              | `torch.bfloat16`  | Brain floating-point format with reduced precision compared to `float16`. Used in mixed-precision training, especially on TPUs.                                                |\n",
        "| **8-bit Floating Point**  | `torch.float8`    | Ultra-low-precision floating point. Used for experimental applications and extreme memory-constrained environments (less common).                                               |\n",
        "| **8-bit Integer**         | `torch.int8`      | 8-bit signed integer. Used for quantized models to save memory and computation in inference.                                                                                   |\n",
        "| **16-bit Integer**        | `torch.int16`     | 16-bit signed integer. Useful for special numerical tasks requiring intermediate precision.                                                                                    |\n",
        "| **32-bit Integer**        | `torch.int32`     | Standard signed integer type. Commonly used for indexing and general-purpose numerical tasks.                                                                                  |\n",
        "| **64-bit Integer**        | `torch.int64`     | Long integer type. Often used for large indexing arrays or for tasks involving large numbers.                                                                                  |\n",
        "| **8-bit Unsigned Integer**| `torch.uint8`     | 8-bit unsigned integer. Commonly used for image data (e.g., pixel values between 0 and 255).                                                                                    |\n",
        "| **Boolean**               | `torch.bool`      | Boolean type, stores `True` or `False` values. Often used for masks in logical operations.                                                                                      |\n",
        "| **Complex 64**            | `torch.complex64` | Complex number type with 32-bit real and 32-bit imaginary parts. Used for scientific and signal processing tasks.                                                               |\n",
        "| **Complex 128**           | `torch.complex128`| Complex number type with 64-bit real and 64-bit imaginary parts. Offers higher precision but uses more memory.                                                                 |\n",
        "| **Quantized Integer**     | `torch.qint8`     | Quantized signed 8-bit integer. Used in quantized models for efficient inference.                                                                                              |\n",
        "| **Quantized Unsigned Integer** | `torch.quint8` | Quantized unsigned 8-bit integer. Often used for quantized tensors in image-related tasks.                                                                                     |\n"
      ],
      "metadata": {
        "id": "nnZMHn2Ro6UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, every tensor has a **data type (`dtype`)**, which defines how numbers are stored and processed. Different data types are used depending on the **precision** and **memory requirements** of a model.\n",
        "\n",
        "## **Checking the Data Type of a Tensor**\n",
        "- Each tensor has a specific `dtype` that can be checked using:\n",
        "  ```python\n",
        "  x.dtype\n",
        "  ```\n",
        "- This returns the data type of `x`, helping you confirm whether the tensor is in **integer**, **floating-point**, or **boolean** format.\n",
        "\n",
        "## **Assigning a Specific Data Type**\n",
        "\n",
        "- When creating a tensor, you can explicitly define its data type using the `dtype` parameter:\n",
        "\n",
        "```python\n",
        "torch.tensor([1.0, 2.0, 3.0], dtype=torch.int32)  # Creates an integer tensor\n",
        "torch.tensor([1, 2, 3], dtype=torch.float64)  # Creates a 64-bit floating-point tensor\n",
        "```\n",
        "\n",
        "## **Changing the Data Type of an Existing Tensor**\n",
        "\n",
        "- PyTorch provides the `.to()` function to convert a tensor to a different data type:\n",
        "\n",
        "```python\n",
        "x.to(torch.float32)  # Converts tensor x to 32-bit floating-point format\n",
        "```\n",
        "\n",
        "## **Commonly Used Data Types in PyTorch**\n",
        "\n",
        "| **Data Type** | **PyTorch dtype** | **Description** |\n",
        "|--------------|-----------------|----------------|\n",
        "| **32-bit Floating Point** | `torch.float32` | Standard type for deep learning, balancing precision and memory. |\n",
        "| **64-bit Floating Point** | `torch.float64` | High precision, but **uses more memory**. |\n",
        "| **16-bit Floating Point** | `torch.float16` | Lower precision, used in **mixed-precision training** to save memory. |\n",
        "| **8-bit Floating Point** | `torch.float8` | Experimental, extremely memory efficient. |\n",
        "| **32-bit Integer** | `torch.int32` | Standard integer format for numerical operations. |\n",
        "| **64-bit Integer** | `torch.int64` | Large integer values, commonly used for tensor indexing. |\n",
        "| **Boolean** | `torch.bool` | Stores `True` or `False` values, often used in masks. |\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **Tensors have a `dtype` that defines how numbers are stored.**  \n",
        "✔ **You can specify the `dtype` when creating a tensor or use `.to()` to convert it later.**  \n",
        "✔ **Choosing the right data type is important for balancing precision and memory usage.**  \n"
      ],
      "metadata": {
        "id": "yb-4NsHc6lr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scalar Operations in PyTorch**"
      ],
      "metadata": {
        "id": "spa-NOZm81k4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scalar Operations in PyTorch**\n",
        "\n",
        "### **What is a Scalar Operation?**\n",
        "- A **scalar operation** is an operation where a **single scalar value** (like `2`, `3`, or `100`) is applied to **every element** in a tensor.\n",
        "- These operations include **addition, subtraction, multiplication, division, modulo, and exponentiation**.\n",
        "\n",
        "### **How do Scalar Operations Work?**\n",
        "- When you perform an operation like `x + 2`, **PyTorch applies the operation to each element** in the tensor.\n",
        "- The tensor **does not change shape**, only the values are modified.\n",
        "\n",
        "---\n",
        "\n",
        "## **Common Scalar Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Addition** | `x + 2` | Adds `2` to each element of `x`. |\n",
        "| **Subtraction** | `x - 2` | Subtracts `2` from each element of `x`. |\n",
        "| **Multiplication** | `x * 3` | Multiplies each element of `x` by `3`. |\n",
        "| **Division** | `x / 3` | Divides each element of `x` by `3`. |\n",
        "| **Integer Division** | `(x * 100) // 3` | Performs floor division after multiplying `x` by `100`. |\n",
        "| **Modulo (Remainder)** | `((x * 100) // 3) % 2` | Computes remainder after integer division. |\n",
        "| **Exponentiation (Power)** | `x ** 2` | Raises each element of `x` to the power of `2`. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Example of Scalar Operations**\n",
        "### **Given Tensor:**\n",
        "```python\n",
        "x = torch.tensor([[0.2, 0.5], [0.8, 1.0]])\n",
        "```\n",
        "\n",
        "## **Operations and Results**\n",
        "\n",
        "| **Operation** | **Formula** | **Result** |\n",
        "|--------------|------------|------------|\n",
        "| **Addition** | `x + 2` | `[[2.2, 2.5], [2.8, 3.0]]` |\n",
        "| **Subtraction** | `x - 2` | `[[-1.8, -1.5], [-1.2, -1.0]]` |\n",
        "| **Multiplication** | `x * 3` | `[[0.6, 1.5], [2.4, 3.0]]` |\n",
        "| **Division** | `x / 3` | `[[0.066, 0.166], [0.266, 0.333]]` |\n",
        "| **Integer Division** | `(x * 100) // 3` | `[[6, 16], [26, 33]]` |\n",
        "| **Modulo** | `((x * 100) // 3) % 2` | `[[0, 0], [0, 1]]` |\n",
        "| **Exponentiation** | `x ** 2` | `[[0.04, 0.25], [0.64, 1.00]]` |\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **Scalar operations apply the same operation to each element in a tensor.**  \n",
        "✔ **The shape of the tensor remains unchanged, only the values are modified.**  \n",
        "✔ **Operations like integer division (`//`) and modulo (`%`) are useful for indexing and discrete calculations.**  \n",
        "✔ **Exponentiation (`**`) is often used in machine learning for transformations and normalization.**  \n"
      ],
      "metadata": {
        "id": "H8p2DMHu84d9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Element-wise Operations in PyTorch**"
      ],
      "metadata": {
        "id": "U8I8BkJo-Ttn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are Element-wise Operations?**\n",
        "- **Element-wise operations** apply mathematical computations to **each corresponding element** in two tensors of the **same shape**.\n",
        "- These operations are **performed independently** on each element without affecting the structure of the tensor.\n",
        "- If two tensors are **not the same shape**, PyTorch applies **broadcasting** to make their shapes compatible.\n",
        "\n",
        "---\n",
        "\n",
        "## **Common Element-wise Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Addition** | `a + b` | Adds corresponding elements of `a` and `b`. |\n",
        "| **Subtraction** | `a - b` | Subtracts corresponding elements of `b` from `a`. |\n",
        "| **Multiplication** | `a * b` | Multiplies corresponding elements of `a` and `b`. |\n",
        "| **Division** | `a / b` | Divides corresponding elements of `a` by `b`. |\n",
        "| **Exponentiation** | `a ** b` | Raises each element of `a` to the power of the corresponding element in `b`. |\n",
        "| **Modulo** | `a % b` | Computes the remainder when each element in `a` is divided by `b`. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Element-wise Absolute and Negation Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Absolute Value** | `torch.abs(c)` | Returns the absolute values of all elements in `c`. |\n",
        "| **Negation** | `torch.neg(c)` | Negates each element of `c` (multiplies by `-1`). |\n",
        "\n",
        "---\n",
        "\n",
        "## **Element-wise Rounding Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Round** | `torch.round(d)` | Rounds each element of `d` to the nearest integer. |\n",
        "| **Ceil (Ceiling)** | `torch.ceil(d)` | Rounds each element of `d` **up** to the nearest integer. |\n",
        "| **Floor** | `torch.floor(d)` | Rounds each element of `d` **down** to the nearest integer. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Clamping Values (Restricting Range)**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Clamp** | `torch.clamp(d, min=2, max=3)` | Limits all values in `d` to be between `2` and `3`. |\n",
        "\n",
        "**Example:**\n",
        "- If `d = torch.tensor([1.9, 2.3, 3.7, 4.4])`\n",
        "- `torch.clamp(d, min=2, max=3)` will return: `tensor([2.0, 2.3, 3.0, 3.0])`\n",
        "\n",
        "- Values **below `2`** are set to `2`, and values **above `3`** are set to `3`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **Element-wise operations are performed on corresponding elements in two tensors of the same shape.**  \n",
        "✔ **Broadcasting allows element-wise operations between tensors of different shapes when possible.**  \n",
        "✔ **Functions like `torch.abs()`, `torch.neg()`, `torch.round()`, and `torch.clamp()` modify individual elements based on mathematical rules.**  \n",
        "✔ **Clamping is useful for restricting values to a certain range, often used in activation functions and normalization.**  \n",
        "\n"
      ],
      "metadata": {
        "id": "6ozOVBt79vrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reduction Operations in PyTorch**"
      ],
      "metadata": {
        "id": "G9oBS3n1_Fb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are Reduction Operations?**\n",
        "- **Reduction operations** compute a **single value** from a tensor by **aggregating** its elements.\n",
        "- These operations **reduce** the number of dimensions by applying mathematical functions like **sum, mean, max, min, standard deviation, and variance**.\n",
        "- You can **reduce an entire tensor to a scalar** or **perform reductions along a specific dimension** using the `dim` parameter.\n",
        "\n",
        "---\n",
        "\n",
        "## **Common Reduction Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Sum** | `torch.sum(e)` | Computes the sum of all elements in `e`. |\n",
        "| **Column-wise Sum** | `torch.sum(e, dim=0)` | Sums elements **along columns** (reducing row dimension). |\n",
        "| **Row-wise Sum** | `torch.sum(e, dim=1)` | Sums elements **along rows** (reducing column dimension). |\n",
        "| **Mean (Average)** | `torch.mean(e)` | Computes the mean (average) of all elements in `e`. |\n",
        "| **Column-wise Mean** | `torch.mean(e, dim=0)` | Computes the mean **along columns**. |\n",
        "| **Median** | `torch.median(e)` | Finds the **median** of all elements in `e`. |\n",
        "| **Max Value** | `torch.max(e)` | Finds the **maximum** value in `e`. |\n",
        "| **Min Value** | `torch.min(e)` | Finds the **minimum** value in `e`. |\n",
        "| **Product** | `torch.prod(e)` | Computes the **product** of all elements in `e`. |\n",
        "| **Standard Deviation** | `torch.std(e)` | Computes the **standard deviation** of all elements. |\n",
        "| **Variance** | `torch.var(e)` | Computes the **variance** of all elements. |\n",
        "| **Index of Maximum Value** | `torch.argmax(e)` | Returns the **index of the max value** in `e`. |\n",
        "| **Index of Minimum Value** | `torch.argmin(e)` | Returns the **index of the min value** in `e`. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Understanding the `dim` Parameter in Reduction Operations**\n",
        "- `dim=0` → **Performs reduction along columns** (collapsing rows).\n",
        "- `dim=1` → **Performs reduction along rows** (collapsing columns).\n",
        "\n",
        "### **Example**\n",
        "Given tensor:\n",
        "```python\n",
        "e = torch.tensor([[3, 7, 1],\n",
        "                  [5, 2, 8]])\n",
        "```\n",
        "\n",
        "## **Operations and Results**\n",
        "\n",
        "| **Operation** | **Formula** | **Result** |\n",
        "|--------------|------------|------------|\n",
        "| **Sum of all elements** | `torch.sum(e)` | `3 + 7 + 1 + 5 + 2 + 8 = 26` |\n",
        "| **Column-wise Sum** | `torch.sum(e, dim=0)` | `[3+5, 7+2, 1+8] = [8, 9, 9]` |\n",
        "| **Row-wise Sum** | `torch.sum(e, dim=1)` | `[3+7+1, 5+2+8] = [11, 15]` |\n",
        "| **Mean of all elements** | `torch.mean(e)` | `26 / 6 = 4.33` |\n",
        "| **Max value** | `torch.max(e)` | `8` |\n",
        "| **Min value** | `torch.min(e)` | `1` |\n",
        "| **Index of Max Value** | `torch.argmax(e)` | `5` (index of `8`) |\n",
        "| **Index of Min Value** | `torch.argmin(e)` | `2` (index of `1`) |\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **Reduction operations help summarize tensor data into a single value or lower-dimensional tensors.**  \n",
        "✔ **Using `dim` allows you to perform reductions along specific axes (columns or rows).**  \n",
        "✔ **Standard deviation (`torch.std`) and variance (`torch.var`) measure the spread of tensor values.**  \n",
        "✔ **`torch.argmax` and `torch.argmin` return the index positions of the max and min values, respectively.**  \n",
        "✔ **These operations are widely used in machine learning for feature aggregation, statistics, and optimization.**  \n",
        "\n"
      ],
      "metadata": {
        "id": "WaDseKM39x11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Matrix Operations in PyTorch**\n",
        "\n",
        "## **What are Matrix Operations?**\n",
        "- Matrix operations are fundamental in **linear algebra** and are widely used in **machine learning**, **deep learning**, and **scientific computing**.\n",
        "- These operations include **matrix multiplication, dot product, transposition, determinant, and inverse**.\n",
        "\n",
        "\n",
        "## **Common Matrix Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Matrix Multiplication** | `torch.matmul(f, g)` | Multiplies two matrices following matrix multiplication rules. |\n",
        "| **Dot Product (Vector Multiplication)** | `torch.dot(vector1, vector2)` | Computes the dot product between two 1D tensors (vectors). |\n",
        "| **Transpose** | `torch.transpose(f, 0, 1)` | Swaps rows and columns of the matrix. |\n",
        "| **Determinant** | `torch.det(h)` | Computes the determinant of a square matrix. |\n",
        "| **Matrix Inverse** | `torch.inverse(h)` | Computes the inverse of a square matrix (if it exists). |\n",
        "\n",
        "\n",
        "## **Understanding Matrix Multiplication (`torch.matmul`)**\n",
        "- **Matrix multiplication** follows the rule:  \n",
        "  - If `A` has shape **(m × n)** and `B` has shape **(n × p)**, then `C = torch.matmul(A, B)` will have shape **(m × p)**.\n",
        "- The number of **columns in the first matrix** must match the **number of rows in the second matrix**.\n",
        "\n",
        "### **Example:**\n",
        "Given:\n",
        "```python\n",
        "f = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "g = torch.tensor([[7, 8],\n",
        "                  [9, 10],\n",
        "                  [11, 12]])\n",
        "```\n",
        "Matrix multiplication `(torch.matmul(f, g))` results in:\n",
        "```python\n",
        "tensor([[ 58,  64],\n",
        "        [139, 154]])\n",
        "\n",
        "```\n",
        "Calculation:\n",
        "\n",
        "- First row: `(1×7) + (2×9) + (3×11)` = `58`, `(1×8) + (2×10) + (3×12)` = `64`\n",
        "- Second row: `(4×7) + (5×9) + (6×11)` = `139`, `(4×8) + (5×10) + (6×12)` = `154`\n",
        "\n",
        "## **Dot Product (`torch.dot`)**\n",
        "\n",
        "### **Definition**\n",
        "- The **dot product** is a mathematical operation that takes **two 1D tensors (vectors)** and returns a **single scalar value**.\n",
        "- It is calculated by multiplying corresponding elements and then summing the results.\n",
        "\n",
        "### **Formula**\n",
        "$$\n",
        "\\text{dot}(a, b) = (a_1 \\times b_1) + (a_2 \\times b_2) + ... + (a_n \\times b_n)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "vector1 = torch.tensor([1, 2])\n",
        "vector2 = torch.tensor([3, 4])\n",
        "\n",
        "result = torch.dot(vector1, vector2)  \n",
        "print(result)  # Output: 1*3 + 2*4 = 11\n",
        "```\n",
        "\n",
        "### **Calculation Breakdown**\n",
        "$$\n",
        "(1 \\times 3) + (2 \\times 4) = 3 + 8 = 11\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "✔ **Dot product applies only to 1D tensors (vectors).**  \n",
        "✔ **It produces a single scalar value.**  \n",
        "✔ **The result represents a weighted sum, commonly used in machine learning, physics, and linear algebra.**  \n",
        "✔ **For higher-dimensional tensors, use `torch.matmul()` or `torch.mm()` instead.**  \n",
        "\n",
        "\n",
        "## **Transpose (`torch.transpose`)**\n",
        "\n",
        "### **Definition**\n",
        "- **Transpose** swaps the **rows and columns** of a matrix.\n",
        "- This is useful in **linear algebra**, **image processing**, and **deep learning** when reshaping data.\n",
        "\n",
        "### **Syntax**\n",
        "```python\n",
        "torch.transpose(tensor, dim0, dim1)\n",
        "```\n",
        "\n",
        "- `dim0`: The first dimension to swap.\n",
        "- `dim1`: The second dimension to swap.\n",
        "\n",
        "### **Example**\n",
        "\n",
        "#### Given Matrix (tensor f):\n",
        "```python\n",
        "f = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "```\n",
        "Matrix `f` looks like:\n",
        "\n",
        "```python\n",
        "tensor([[1, 2, 3],\n",
        "        [4, 5, 6]])\n",
        "```\n",
        "\n",
        "#### **Applying Transpose:**\n",
        "\n",
        "```pyhton\n",
        "torch.transpose(f, 0, 1)\n",
        "```\n",
        "\n",
        "\n",
        "This swaps rows `(dim0=0)` and columns `(dim1=1)`, resulting in:\n",
        "```python\n",
        "tensor([[1, 4],\n",
        "        [2, 5],\n",
        "        [3, 6]])\n",
        "```\n",
        "\n",
        "### **Key Takeaways**\n",
        "✔ **Transpose (`torch.transpose`) swaps matrix dimensions without changing data values.**  \n",
        "✔ **It is essential in reshaping data for matrix operations like multiplication and neural networks.**  \n",
        "✔ **Used frequently in image processing** (e.g., converting `Height × Width × Channels` to `Channels × Height × Width`).  \n",
        "✔ **Equivalent to `.T` for 2D tensors (`tensor.T`).**  \n",
        "\n",
        "\n",
        "## **Determinant (`torch.det`)**\n",
        "\n",
        "### **Definition**\n",
        "- The **determinant** is a **scalar value** that describes properties of a **square matrix** (e.g., whether it has an **inverse**).\n",
        "- Only works for **square matrices** of shape `(n × n)`.\n",
        "- If the determinant of a matrix is **zero**, the matrix **does not have an inverse** (i.e., it is singular).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "h = torch.tensor([[2, 3, 1],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]], dtype=torch.float32)\n",
        "\n",
        "result = torch.det(h)\n",
        "print(result)  # Output: -3.0\n",
        "```\n",
        "Given Matrix (h):\n",
        "\n",
        "```python\n",
        "tensor([[2, 3, 1],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9]])\n",
        "```\n",
        "### **Determinant Calculation**\n",
        "$$\n",
        "(2 \\times 5 \\times 9) + (3 \\times 6 \\times 7) + (1 \\times 4 \\times 8) -\n",
        "(1 \\times 5 \\times 7) - (3 \\times 4 \\times 9) - (2 \\times 6 \\times 8) = -3\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "✔ **The determinant is a single scalar value computed from a square matrix.**  \n",
        "✔ **If `det(A) = 0`, the matrix `A` does not have an inverse (it is singular).**  \n",
        "✔ **Used in linear algebra for solving equations, checking invertibility, and understanding transformations.**  \n",
        "✔ **Only works for square matrices (`n × n`).**  \n",
        "\n",
        "## **Matrix Inverse (`torch.inverse`)**\n",
        "\n",
        "### **Definition**\n",
        "- The **inverse of a matrix** `A` is a matrix `A⁻¹` such that:\n",
        "  $$\n",
        "  A \\times A^{-1} = I\n",
        "  $$\n",
        "  where `I` is the **identity matrix**.\n",
        "- The inverse exists **only for square, non-singular matrices** (i.e., matrices where `det(A) ≠ 0`).\n",
        "- If `det(A) = 0`, the matrix is **singular**, and `torch.inverse(A)` will fail.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "h = torch.tensor([[4, 7],\n",
        "                  [2, 6]], dtype=torch.float32)\n",
        "\n",
        "inverse_h = torch.inverse(h)\n",
        "print(inverse_h)\n",
        "```\n",
        "\n",
        "Given Matrix (`h`):\n",
        "```python\n",
        "tensor([[4, 7],\n",
        "        [2, 6]])\n",
        "```\n",
        "Inverse of `h` (`h⁻¹`):\n",
        "```python\n",
        "tensor([[ 0.6, -0.7],\n",
        "        [-0.2,  0.4]])\n",
        "```\n",
        "\n",
        "### **Formula for Matrix Inversion**\n",
        "$$\n",
        "A^{-1} = \\frac{1}{\\det(A)} \\times \\text{adj}(A)\n",
        "$$\n",
        "where:\n",
        "- **`det(A)`** is the **determinant** of `A`.\n",
        "- **`adj(A)`** is the **adjugate matrix** (cofactor matrix transposed).\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "✔ **The inverse of a matrix is a matrix that, when multiplied with the original, gives the identity matrix.**  \n",
        "✔ **Only square matrices (`n × n`) with a nonzero determinant have an inverse.**  \n",
        "✔ **If `torch.det(A) == 0`, `torch.inverse(A)` will fail because singular matrices do not have an inverse.**  \n",
        "✔ **Matrix inversion is used in solving systems of equations, cryptography, and transformations in linear algebra.**  \n",
        "\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **Matrix multiplication (`torch.matmul`) follows standard linear algebra rules and is essential in deep learning models.**  \n",
        "✔ **Dot product (`torch.dot`) applies only to 1D tensors and computes a scalar result.**  \n",
        "✔ **Transpose (`torch.transpose`) swaps matrix dimensions, useful for reshaping data.**  \n",
        "✔ **Determinant (`torch.det`) helps check if a matrix is invertible.**  \n",
        "✔ **Matrix inverse (`torch.inverse`) exists only for non-singular, square matrices.**  \n",
        "\n"
      ],
      "metadata": {
        "id": "p7bdqgpQ_yxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In-place Operations in PyTorch**"
      ],
      "metadata": {
        "id": "juP5mfnT_yuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **What are In-place Operations?**\n",
        "- **In-place operations modify the original tensor instead of creating a new one.**\n",
        "- They **save memory** by avoiding the creation of new tensors but **should be used cautiously** to prevent unintended changes.\n",
        "- In PyTorch, **in-place operations** are denoted by an **underscore (`_`)** at the end of the function name (e.g., `add_()`, `relu_()`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Common In-place Operations**\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **In-place Addition** | `m.add_(n)` | Adds `n` to `m` and modifies `m` directly. |\n",
        "| **In-place ReLU Activation** | `m.relu_()` | Applies the ReLU function to `m`, replacing negative values with `0`. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Example of In-place Operations**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create two random tensors\n",
        "m = torch.rand(2,3)\n",
        "n = torch.rand(2,3)\n",
        "\n",
        "print(\"Original Tensor m:\\n\", m)\n",
        "print(\"Tensor n:\\n\", n)\n",
        "\n",
        "# In-place addition (m is modified)\n",
        "m.add_(n)\n",
        "print(\"Tensor m after in-place addition:\\n\", m)\n",
        "\n",
        "# Applying ReLU (not in-place)\n",
        "torch.relu(m)  # This does NOT modify m\n",
        "\n",
        "# Applying ReLU in-place (m is modified)\n",
        "m.relu_()\n",
        "print(\"Tensor m after in-place ReLU:\\n\", m)\n",
        "```\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **In-place operations modify tensors directly and save memory.**  \n",
        "✔ **They are denoted by an underscore (`_`) at the end of the function name** (e.g., `add_()`, `relu_()`).  \n",
        "✔ **They should be used with caution, as modifying tensors in-place can cause issues in computational graphs (autograd).**  \n",
        "✔ **For non-in-place operations, use functions without `_`, like `torch.add()` and `torch.relu()`.**  \n"
      ],
      "metadata": {
        "id": "Cmw3W12R_yrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Copying a Tensor in PyTorch**"
      ],
      "metadata": {
        "id": "u9GHYv0q_yo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **What Happens When You Assign One Tensor to Another?**\n",
        "- In PyTorch, if you assign a tensor to another variable **without cloning it**, both variables **point to the same memory location**.\n",
        "- Modifying one tensor **affects the other**, as they are just references to the same data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Types of Tensor Copying**\n",
        "\n",
        "| **Operation** | **Expression** | **Description** |\n",
        "|--------------|--------------|----------------|\n",
        "| **Direct Assignment** | `b = a` | `b` and `a` point to the same memory. Changes in `a` affect `b`. |\n",
        "| **Cloning (Deep Copy)** | `b = a.clone()` | Creates an independent copy of `a`. Changes in `a` do **not** affect `b`. |\n",
        "| **Checking Memory Location** | `id(a), id(b)` | Used to verify whether two tensors share the same memory. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Example: Direct Assignment vs Cloning**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a random tensor\n",
        "a = torch.rand(2,3)\n",
        "print(\"Original Tensor a:\\n\", a)\n",
        "```\n",
        "\n",
        "```python\n",
        "Original Tensor a:\n",
        " tensor([[0.7140, 0.4896, 0.0917],\n",
        "        [0.3454, 0.6442, 0.4404]])\n",
        "```\n",
        "\n",
        "```python\n",
        "# Direct assignment (both point to the same memory)\n",
        "b = a\n",
        "print(\"Tensor b after assignment:\\n\", b)\n",
        "```\n",
        "\n",
        "```python\n",
        "Tensor b after assignment:\n",
        " tensor([[0.7140, 0.4896, 0.0917],\n",
        "        [0.3454, 0.6442, 0.4404]])\n",
        "```\n",
        "\n",
        "```python\n",
        "# Modifying 'a' affects 'b'\n",
        "a[0][0] = 0\n",
        "print(\"Tensor a after modification:\\n\", a)\n",
        "print(\"Tensor b after modification:\\n\", b)  # b is also modified\n",
        "```\n",
        "\n",
        "```python\n",
        "Tensor a after modification:\n",
        " tensor([[0.0000, 0.4896, 0.0917],\n",
        "        [0.3454, 0.6442, 0.4404]])\n",
        "\n",
        "Tensor b after modification:\n",
        " tensor([[0.0000, 0.4896, 0.0917],\n",
        "        [0.3454, 0.6442, 0.4404]])\n",
        "```\n",
        "\n",
        "```python\n",
        "# Checking memory addresses\n",
        "print(\"Memory ID of a:\", id(a))\n",
        "print(\"Memory ID of b:\", id(b))  # Same memory ID\n",
        "```\n",
        "\n",
        "```python\n",
        "Memory ID of a: 132887038379696\n",
        "Memory ID of b: 132887038379696\n",
        "```\n",
        "\n",
        "```python\n",
        "# Cloning a tensor (creating a separate copy)\n",
        "b = a.clone()\n",
        "print(\"Tensor b after cloning:\\n\", b)\n",
        "```\n",
        "\n",
        "```python\n",
        "Tensor b after cloning:\n",
        " tensor([[0.0000, 0.4896, 0.0917],\n",
        "        [0.3454, 0.6442, 0.4404]])\n",
        "```\n",
        "\n",
        "```python\n",
        "# Modifying 'a' does not affect 'b' anymore\n",
        "a[0][0] = 10\n",
        "print(\"Tensor a after modifying cloned version:\\n\", a)\n",
        "print(\"Tensor b remains unchanged:\\n\", b)  # b is now independent\n",
        "```\n",
        "\n",
        "```python\n",
        "Tensor a after modifying cloned version:\n",
        " tensor([[10.0000,  0.4896,  0.0917],\n",
        "        [ 0.3454,  0.6442,  0.4404]])\n",
        "Tensor b remains unchanged:\n",
        " tensor([[0.0000, 0.4896, 0.0917],\n",
        "        [0.3454, 0.6442, 0.4404]])\n",
        "```\n",
        "\n",
        "```python\n",
        "# Checking memory addresses\n",
        "print(\"Memory ID of a:\", id(a))\n",
        "print(\"Memory ID of b:\", id(b))  # Different memory ID now\n",
        "```\n",
        "\n",
        "```python\n",
        "Memory ID of a: 132887038379696\n",
        "Memory ID of b: 132887038386416\n",
        "```\n",
        "\n",
        "\n",
        "## **Key Takeaways**\n",
        "✔ **Direct assignment (`b = a`) makes both variables reference the same memory, so modifying one affects the other.**  \n",
        "✔ **Using `a.clone()` creates a separate copy of `a`, allowing independent modifications.**  \n",
        "✔ **To check if two tensors share the same memory, use `id(a) == id(b)`.**  \n",
        "✔ **Cloning is essential when working with models and gradient updates to avoid unintended modifications.**  \n"
      ],
      "metadata": {
        "id": "ZwTvKXld_ymF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BqpD5OfT9zQb"
      }
    }
  ]
}